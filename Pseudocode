# Micks thoughts from https://twitter.com/BioMickWatson/status/1224983069490339840
#1) upload metagenomics reads to NCBI

#2) Mash screen automatically tells me which genomes are in my reads

#3) Automatic bulk download of those genomes

#4) Build (for example) Kraken DB from them

#5) Taxonomic profile of reads

# Mash screen example from https://mash.readthedocs.io/en/latest/tutorials.html
# need to download mash database first: 
# wget -c https://gembox.cbcb.umd.edu/mash/refseq.genomes%2Bplasmid.k21s1000.msh
# Test data recommended by docs
# fastq-dump ERR024951


mash screen -w -p 4 refseq.genomes+plasmid.k21s1000.msh ERR024951.fastq > screen.tab
# -w is for 'winner takes all' to reduce reduncancy
# -p is for threads (change for scaling)

sort -g -r screen.tab | cut -f 5
# pull out just the genomes

# take first 3, 5-7, 8-10, 11-13, trim after third underscore and add all to end
wget -c "ftp://ftp.ncbi.nlm.nih.gov/genomes/all/${ref:0:3}/${ref:4:3}/${ref:7:3}/${ref:10:3}/$(echo $ref | cut -d _ -f 1-3)/$ref"

ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/068/585/GCF_000068585.1_ASM6858v1/GCF_000068585.1_ASM6858v1_genomic.fna.gz



sed 's/ref\|//'
sed 's/\|//'
sed 's///'

date > log.txt; cat downloads.txt | parallel --gnu "echo {}; wget {} 2>&1 | grep -i 'failed\|error' >> log.txt";
# took 30 minutes, generated ~5 GB of data

# add genomes to kraken2 database, either remove intermediate files, or keep at least the gzip files
basename -s '.fna.gz' *.fna.gz | parallel -j 8 --gnu "gunzip {}.fna.gz; kraken2-build --add-to-libary {}.fna --db ../dumplinglib; rm {}.fna; rm {}.fna.gz"
basename -s '.fna.gz' *.fna.gz | parallel -j 8 --gnu "gunzip {}.fna.gz; kraken2-build --add-to-libary {}.fna --db ../dumplinglib; rm {}.fna"

# May need to also download the taxonomy info from NCBI as per kraken2 info
kraken2-build --download-taxonomy --threads 8 --db ../dumplinglib
#generates 30 gb of data...quite a lot

kraken2-build --build --threads 8 --db ../dumplinglib

kraken2 --db $DBNAME --threads NUM seqs.fa
